# Transformer from Scratch (å­¦ä¹ ç‰ˆ)

æœ¬ä»“åº“å†…å®¹åŸºäº [Wayland Zhang](https://github.com/waylandzhang) è€å¸ˆçš„å¼€æºä»“åº“å’Œè¯¾ç¨‹è§†é¢‘æ•´ç†å­¦ä¹ ã€‚  
åŸå§‹å®ç°è¯·å‚è€ƒï¼š[nanoGPT](https://github.com/karpathy/nanoGPT) å’Œ [å¼ è€å¸ˆçš„ä»“åº“](https://github.com/waylandzhang/transformer-from-scratch)ã€‚  

> âš ï¸ æœ¬ä»“åº“ä»…ä½œä¸ªäººå­¦ä¹ ä¸è¯¾ç¨‹è·Ÿç»ƒä½¿ç”¨ï¼Œä¸æ¶‰åŠåŸåˆ›è´¡çŒ®ï¼Œå¦‚éœ€ä½¿ç”¨æˆ–å¼•ç”¨ï¼Œè¯·ä»¥åŸä»“åº“ä¸ºå‡†ã€‚

---

## é¡¹ç›®ç®€ä»‹

è¿™æ˜¯ä¸€ä¸ª **Transformer æ¶æ„çš„ Large Language Model (LLM)** è®­ç»ƒ Demoï¼Œä»…ä½¿ç”¨ _çº¦ 240 è¡Œä»£ç _ã€‚  

é€šè¿‡è¯¥ Demoï¼Œå¯ä»¥ä»é›¶å¼€å§‹ç†è§£å¦‚ä½•ç”¨ PyTorch è®­ç»ƒä¸€ä¸ªç®€å•çš„ LLMã€‚  
ä»£ç ç®€æ´æ˜“æ‡‚ï¼Œé€‚åˆä½œä¸ºå…¥é—¨å­¦ä¹ ææ–™ã€‚

- è®­ç»ƒæ•°æ®ï¼šçº¦ 450 KB [sample textbook](https://huggingface.co/datasets/goendalf666/sales-textbook_for_convincing_and_selling/raw/main/sales_textbook.txt)  
- æ¨¡å‹å¤§å°ï¼šçº¦ 51M  
- å‚æ•°é‡ï¼šçº¦ 1.3M  
- ç¡¬ä»¶ï¼šå•å° i7 CPU  
- è®­ç»ƒæ—¶é—´ï¼šçº¦ 20 åˆ†é’Ÿ

---

## ğŸš€ å¿«é€Ÿå¼€å§‹

### 1. å®‰è£…ä¾èµ–

```bash
pip install numpy requests torch tiktoken

### 2. è¿è¡Œæ¨¡å‹è®­ç»ƒ

```bash
python model.py

- ç¬¬ä¸€æ¬¡è¿è¡Œä¼šè‡ªåŠ¨ä¸‹è½½æ•°æ®é›†å¹¶ä¿å­˜åˆ° data æ–‡ä»¶å¤¹ã€‚

- æ¨¡å‹å°†åœ¨æ•°æ®é›†ä¸Šå¼€å§‹è®­ç»ƒï¼Œå¹¶åœ¨æ§åˆ¶å°è¾“å‡ºè®­ç»ƒä¸éªŒè¯ Lossï¼Œä¾‹å¦‚ï¼š

Step: 0 Training Loss: 11.68 Validation Loss: 11.681
Step: 20 Training Loss: 10.322 Validation Loss: 10.287
Step: 40 Training Loss: 8.689 Validation Loss: 8.783
...

- 5000 æ¬¡è¿­ä»£åï¼ŒLoss ä¼šä¸‹é™åˆ°çº¦ 2.807ï¼Œæ¨¡å‹ä¼šä¿å­˜ä¸º model-ckpt.ptã€‚
- è®­ç»ƒå®Œæˆåï¼Œä¼šåœ¨æ§åˆ¶å°è¾“å‡ºæ¨¡å‹ç”Ÿæˆæ–‡æœ¬ç¤ºä¾‹ï¼Œä¾‹å¦‚ï¼š
The salesperson to identify the other cost savings interaction towards a nextProps audience, ...
æç¤ºï¼šå¯ä»¥ä¿®æ”¹ model.py é¡¶éƒ¨çš„è¶…å‚æ•°ï¼Œè§‚å¯Ÿè®­ç»ƒæ•ˆæœçš„å˜åŒ–ã€‚

### Step-by-Step Notebook

æœ¬ä»“åº“æä¾› step-by-step.ipynbï¼Œé€æ­¥å±•ç¤º Transformer çš„è®¡ç®—è¿‡ç¨‹ã€‚
è¿è¡Œå‰éœ€è¦å®‰è£…é¢å¤–ä¾èµ–ï¼š

```bash
pip install matplotlib pandas

Notebook ä¸­åŒ…å«ï¼š

- [x] è¾“å…¥åµŒå…¥çŸ©é˜µç¤ºä¾‹  
- [x] ä½ç½®ç¼–ç å¯è§†åŒ–  
- [x] æ³¨æ„åŠ›çŸ©é˜µä¸ Mask æ“ä½œå¯è§†åŒ–  

ğŸ‘‰ å¸®åŠ©ç†è§£ Transformer **Decoder-only æ¶æ„** çš„è®­ç»ƒæµç¨‹ã€‚


æ³¨æ„åŠ›çŸ©é˜µä¸ Mask æ“ä½œå¯è§†åŒ– å¸®åŠ©ç†è§£ Transformer Decoder-only æ¶æ„ çš„è®­ç»ƒæµç¨‹ã€‚ 
å…¶å®ƒå†…å®¹ åœ¨ /GPT2 ç›®å½•ä¸‹ï¼Œæœ‰ä¸€äº›ç¤ºä¾‹ä»£ç æ¼”ç¤ºå¦‚ä½•å¾®è°ƒé¢„è®­ç»ƒ GPT2 æ¨¡å‹å¹¶è¿›è¡Œæ¨ç†ã€‚

## æ¨èé˜…è¯»

- [nanoGPT](https://github.com/karpathy/nanoGPT) â€” Andrej Karpathy çš„ç»å…¸ GPT æ•™ç¨‹  
- [Transformers from Scratch](https://blog.matdmiller.com/posts/2023-06-10_transformers/notebook.html) â€” Mat Miller çš„ç®€æ´å®ç°  
- [Attention is All You Need](https://arxiv.org/abs/1706.03762) â€” Transformer åŸå§‹è®ºæ–‡  
- [Transformer Architecture: LLM From Zero-to-Hero](https://medium.com/@waylandzhang/transformer-architecture-llms-zero-to-hero-98b1ee51a838) â€” å¼ è€å¸ˆçš„è®²è§£æ–‡ç«   

